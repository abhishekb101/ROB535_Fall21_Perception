{"cells":[{"cell_type":"markdown","metadata":{"id":"1oA1PQbjZEfL","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["### Section I: Setup"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"cH_b_4jUb5Zg"},"source":["#### Importing libraries required"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMkxzDoOb5Zh"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import os, sys\n","from glob import glob\n","from mpl_toolkits.mplot3d import Axes3D\n","import random\n","from PIL import Image\n","from tqdm import tqdm\n","from tqdm.notebook import tqdm_notebook\n","import tensorflow\n","import zipfile\n","import tensorflow_hub as hub\n","import sys\n","np.set_printoptions(threshold=sys.maxsize)"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"c_--UxkDb5Zj"},"source":["#### Extracting relevant zip files for running models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HgFK3o2Bb5Zk"},"outputs":[],"source":["local_zip = \"Task1.zip\"\n","with zipfile.ZipFile(local_zip, 'r') as image_zip:\n","    image_zip.extractall()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvYZZ43Zb5Zk","outputId":"73b87b33-9b15-4b46-c916-7e58504584f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unzipped files!!\n"]}],"source":["zip_file = \"Preprocessed_subset_Task1.zip\"\n","with zipfile.ZipFile(zip_file, 'r') as preprocessed_image_zip:\n","    preprocessed_image_zip.extractall()\n","print(\"Unzipped files!!\")"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"BCUs_pYVb5Zm"},"source":["### Section II: EDA and reference functions"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"vUy8yak0b5Zm"},"source":["#### Expolatory Data Analysis "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRJi48pkaSti"},"outputs":[],"source":["#Demo code to read data\n","def rot(n):\n","    \"\"\"\n","    Taking a rotation vector n and returning the resultant rotation matrix\n","    \"\"\"\n","    n = np.asarray(n).flatten()\n","    assert(n.size == 3)\n","\n","    theta = np.linalg.norm(n)\n","    if theta:\n","        n /= theta\n","        K = np.array([[0, -n[2], n[1]], [n[2], 0, -n[0]], [-n[1], n[0], 0]])\n","        return np.identity(3) + np.sin(theta) * K + (1 - np.cos(theta)) * K @ K\n","    else:\n","        return np.identity(3)\n","\n","def get_bbox(p0, p1):\n","    \"\"\"\n","    Input:\n","    * p0, p1\n","    (3)\n","    Corners of a bounding box represented in the body frame.\n","\n","    Output:\n","    *   v\n","    (3, 8)\n","    Vertices of the bounding box represented in the body frame.\n","    * e\n","    (2, 14)\n","    Edges of the bounding box. The first 2 edges indicate the `front` side\n","    of the box.\n","    \"\"\"\n","    v = np.array([\n","        [p0[0], p0[0], p0[0], p0[0], p1[0], p1[0], p1[0], p1[0]],\n","        [p0[1], p0[1], p1[1], p1[1], p0[1], p0[1], p1[1], p1[1]],\n","        [p0[2], p1[2], p0[2], p1[2], p0[2], p1[2], p0[2], p1[2]]])\n","  \n","    e = np.array([\n","        [2, 3, 0, 0, 3, 3, 0, 1, 2, 3, 4, 4, 7, 7],\n","        [7, 6, 1, 2, 1, 2, 4, 5, 6, 7, 5, 6, 5, 6]], dtype=np.uint8)\n","\n","    return v, e\n","\n","#Printing list of classes from classes.csv file\n","classes = (\n","    'Unknown', 'Compacts', 'Sedans', 'SUVs', 'Coupes',\n","    'Muscle', 'SportsClassics', 'Sports', 'Super', 'Motorcycles',\n","    'OffRoad', 'Industrial', 'Utility', 'Vans', 'Cycles',\n","    'Boats', 'Helicopters', 'Planes', 'Service', 'Emergency',\n","    'Military', 'Commercial', 'Trains')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79rJLjrkhod6"},"outputs":[],"source":["df = pd.read_csv(\"classes.csv\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NpvmAunIb5Zo"},"outputs":[],"source":["#Getting point cloud for each image \n","xyz = np.fromfile(snapshot.replace('_image.jpg', '_cloud.bin'), dtype=np.float32)\n","xyz = xyz.reshape([3, -1])\n","\n","#Getting the camera projection matrix 'M' for each image \n","proj = np.fromfile(snapshot.replace('_image.jpg', '_proj.bin'), dtype=np.float32)\n","proj.resize([3, 4])\n","\n","#Getting bounding box for each image\n","try:\n","    bbox = np.fromfile(snapshot.replace('_image.jpg', '_bbox.bin'), dtype=np.float32)\n","except FileNotFoundError:\n","    print('[*] bbox not found.')\n","    bbox = np.array([], dtype=np.float32)\n","\n","bbox = bbox.reshape([-1, 11])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9elV1keqhXwy","jupyter":{"outputs_hidden":true},"tags":[]},"outputs":[],"source":["uv = proj @ np.vstack([xyz, np.ones_like(xyz[0, :])])\n","uv = uv / uv[2, :]\n","\n","dist = np.linalg.norm(xyz, axis=0)\n","fig1 = plt.figure(1, figsize=(10, 7))\n","ax1 = fig1.add_subplot(1, 1, 1)\n","ax1.imshow(img)\n","ax1.scatter(uv[0, :], uv[1, :], c=dist, marker='+', s=1)\n","ax1.axis('scaled')\n","fig1.tight_layout()\n","\n","fig2 = plt.figure(2, figsize=(8, 8))\n","ax2 = Axes3D(fig2)\n","ax2.set_xlabel('x')\n","ax2.set_ylabel('y')\n","ax2.set_zlabel('z')\n","\n","step = 5\n","ax2.scatter(\n","    xyz[0, ::step], xyz[1, ::step], xyz[2, ::step],\n","    c=dist[::step], marker='.', s=1\n",")\n","\n","colors = ['C{:d}'.format(i) for i in range(10)]\n","for k, b in enumerate(bbox):\n","    R = rot(b[0:3])\n","    t = b[3:6]\n","\n","    sz = b[6:9]\n","    vert_3D, edges = get_bbox(-sz / 2, sz / 2)\n","    vert_3D = R @ vert_3D + t[:, np.newaxis]\n","\n","    vert_2D = proj @ np.vstack([vert_3D, np.ones(vert_3D.shape[1])])\n","    vert_2D = vert_2D / vert_2D[2, :]\n","\n","    clr = colors[np.mod(k, len(colors))]\n","    for e in edges.T:\n","        ax1.plot(vert_2D[0, e], vert_2D[1, e], color=clr)\n","        ax2.plot(vert_3D[0, e], vert_3D[1, e], vert_3D[2, e], color=clr)\n","\n","    c = classes[int(b[9])]\n","    ignore_in_eval = bool(b[10])\n","    if ignore_in_eval:\n","        ax2.text(t[0], t[1], t[2], c, color='r')\n","    else:\n","        ax2.text(t[0], t[1], t[2], c)\n","\n","ax2.auto_scale_xyz([-40, 40], [-40, 40], [0, 80])\n","ax2.view_init(elev=-30, azim=-90)\n","\n","for e in np.identity(3):\n","    ax2.plot([0, e[0]], [0, e[1]], [0, e[2]], color=e)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4gqX77kk9iJ","jupyter":{"outputs_hidden":true},"tags":[]},"outputs":[],"source":["#Plotting multiple images in the test set\n","# Set up matplotlib fig, and size it to fit 4x4 pics\n","def plot_multiple_images(nrows, ncols, image_path_files):\n","    ncols = ncols\n","    nrows = nrows\n","    fig = plt.gcf()\n","    fig.set_size_inches(14, 10)\n","    idx_list = random.sample(range(1, len(image_path_files)), 20)\n","\n","    car_pix = [image_path_files[idx] for idx in idx_list]\n","\n","    for i, img_path in tqdm_notebook(enumerate(car_pix)):\n","        # Set up subplot; subplot indices start at 1\n","        sp = plt.subplot(nrows, ncols, i + 1)\n","        sp.axis('Off') # Don't show axes (or gridlines)\n","        img = plt.imread(img_path)\n","        plt.imshow(img)\n","\n","    plt.show()\n","\n","plot_multiple_images(4, 5, files)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CySie53Zb5Zq"},"outputs":[],"source":["zip_file = \"Preprocessed_Task1.zip\"\n","with zipfile.ZipFile(zip_file, 'r') as preprocessed_image_zip:\n","    preprocessed_image_zip.extractall()\n","print(\"Unzipped files!!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y1Cm5z0Pb5Zq"},"outputs":[],"source":["base_dir2 = os.path.join(os.getcwd(), \"Preprocessed_Task1\")\n","train_unknown_dir = os.path.join(base_dir2, \"Unknown_and_Others\")\n","train_car_dir = os.path.join(base_dir2, \"Cars\")\n","train_2wheeler_dir = os.path.join(base_dir2, \"2_Wheeler\")\n","train_offroad_dir = os.path.join(base_dir2, \"Offroad\")\n","train_industrial_dir = os.path.join(base_dir2, \"Industrial\")\n","train_utility_dir = os.path.join(base_dir2, \"Utility\")\n","train_service_dir = os.path.join(base_dir2, \"Service\")\n","train_commercial_dir = os.path.join(base_dir2, \"Commercial\")\n","#Printing results to crosscheck successful file transfer\n","print(\"The number of Class: {} = {}\".format(\"Unknown_and_Others\", len(os.listdir(train_unknown_dir))))\n","print(\"The number of Class: {} = {}\".format(\"Cars\", len(os.listdir(train_car_dir))))\n","print(\"The number of Class: {} = {}\".format(\"2_Wheeler\", len(os.listdir(train_2wheeler_dir))))\n","print(\"The number of Class: {} = {}\".format(\"Offroad\", len(os.listdir(train_offroad_dir))))\n","print(\"The number of Class: {} = {}\".format(\"Industrial\", len(os.listdir(train_industrial_dir))))\n","print(\"The number of Class: {} = {}\".format(\"Utility\", len(os.listdir(train_utility_dir))))\n","print(\"The number of Class: {} = {}\".format(\"Service\", len(os.listdir(train_service_dir))))\n","print(\"The number of Class: {} = {}\".format(\"Commercial\", len(os.listdir(train_commercial_dir))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LVL7tx4ub5Zr"},"outputs":[],"source":["from glob import glob\n","print(\"Total number of training images = {}\".format(len(glob(\"Preprocessed_Task1/*/*.jpg\"))))"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"pxNUNklVb5Zr"},"source":["#### Reference functions and checking GPU instance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fOQE2WQO8MM8"},"outputs":[],"source":["#Creating mapping dictionaries for output label class and class names\n","id2label_dict = {id:int(df[df.class_id == id].label) for id in df.class_id}\n","label2classes_dict = {0: \"Unknown_and_Others\",\n","                      1: \"Cars\",\n","                      2: \"Other_modes_of_transport\"}\n","\n","#Reference functions to copy images into their respective directories\n","def img2classid(img_path):\n","    bbox = np.fromfile(img_path.replace('_image.jpg', '_bbox.bin'), dtype=np.float32)\n","    bbox = bbox.reshape([-1, 11])\n","    class_id = int(bbox[:,-2])\n","    return id2label_dict[class_id]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xr7MGyI2b5Zs"},"outputs":[],"source":["#Checking GPU available\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","    print('Not connected to a GPU')\n","else:\n","    print(gpu_info)\n","    \n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","    print('Not using a high-RAM runtime')\n","else:\n","    print('You are using a high-RAM runtime!')"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"J_c0mXmPb5Zs"},"source":["#### Loading images using ImageDataGenerator class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TzFIu9vb5Zs","outputId":"537b8569-56ee-412e-8027-835b5a29f767"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 6441 images belonging to 8 classes.\n","Found 1132 images belonging to 8 classes.\n"]}],"source":["#Creating ImageDataGenerators for training, validation sets\n","base_dir2 = os.path.join(os.getcwd(), \"Preprocessed_Task1\")\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","res_size = 224\n","classes_ls = [\"Unknown_and_Others\", \"Cars\", \"2_Wheeler\", \"Offroad\", \"Industrial\", \"Utility\", \"Service\", \"Commercial\"]\n","\n","train_datagen = ImageDataGenerator(height_shift_range=0.15, fill_mode='nearest',\n","                                   horizontal_flip=True, rescale=1.0/255., validation_split=0.15)\n","\n","train_generator = train_datagen.flow_from_directory(\n","                  base_dir2, target_size=(res_size, res_size), color_mode='rgb',\n","                  classes = classes_ls,\n","                  class_mode='categorical', batch_size=32, shuffle=True,\n","                  subset=\"training\", interpolation='bilinear')\n","\n","val_generator = train_datagen.flow_from_directory(\n","                base_dir2, target_size=(res_size, res_size), color_mode='rgb',\n","                classes = classes_ls,\n","                class_mode='categorical', batch_size=32, shuffle=True,\n","                subset=\"validation\", interpolation='bilinear')"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"eUdo6xn-b5Zt"},"source":["#### Fine tuning a ResNetV2 50 model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vHxkx-TSb5Zt"},"outputs":[],"source":["##Leveraging training on Google Colab pro with starting 130 layers frozen of ResNetV2 50\n","from tensorflow.keras.models import load_model\n","checkpoint_path = \"training_ResNetV2-005.h5\"\n","resnetv2_model = load_model(checkpoint_path)\n","resnetv2_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_FT8mlaab5Zt"},"outputs":[],"source":["#Compiling the model\n","from tensorflow.keras.optimizers import Adam\n","opt = Adam(learning_rate = 2e-3)\n","loss = tensorflow.keras.losses.CategoricalCrossentropy()\n","\n","resnetv2_model.compile(optimizer = opt, \n","                       loss = loss, \n","                       metrics = [\"accuracy\"])\n","\n","lor = tf.keras.callbacks.ReduceLROnPlateau(\n","    monitor='val_accuracy', factor=0.3, patience=4, verbose=1,\n","    mode='auto', min_delta=0.1, min_lr=0)\n","\n","checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","    \"ResNetV2_training/training-{epoch:03d}-{val_loss:04f}-{val_accuracy:04f}.h5\", monitor='val_accuracy', verbose=1, save_best_only=True,\n","    save_weights_only = False, save_freq='epoch')\n","\n","with tf.device('/gpu:0'):\n","    history2 = resnetv2_model.fit(\n","               train_generator,\n","               validation_data = val_generator,\n","               epochs = 55, \n","               verbose = 1, shuffle=True,\n","               callbacks = [lor, checkpoint])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QbWO-pT6b5Zt"},"outputs":[],"source":["acc = history2.history['accuracy']\n","val_acc = history2.history['val_accuracy']\n","loss = history2.history['loss']\n","val_loss = history2.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'r', label='Training accuracy')\n","plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.legend(loc=0)\n","plt.figure()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EStpmqtUb5Zu"},"outputs":[],"source":["acc = history2.history['accuracy']\n","val_acc = history2.history['val_accuracy']\n","loss = history2.history['loss']\n","val_loss = history2.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'r', label='Training accuracy')\n","plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.legend(loc=0)\n","plt.figure()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"Sdu2qiA1b5Zu"},"source":["#### Fine Training ResNetv250 with more data augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVaJBcRBb5Zu"},"outputs":[],"source":["##Leveraging training on Google Colab pro with starting 130 layers frozen of ResNetV2 50\n","from tensorflow.keras.models import load_model\n","checkpoint_path = \"ResNetV2_training/training_ResNetV2-046.h5\"\n","resnetv2_model = load_model(checkpoint_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"En1WQDFgb5Zu"},"outputs":[],"source":["#Only th first 90 layers are frozen\n","for layer in resnetv2_model.layers[90:130]:\n","    layer.trainable = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcbSz_vxb5Zu","outputId":"bf057812-366a-4838-de5f-d4b3025c39c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 6441 images belonging to 8 classes.\n","Found 1132 images belonging to 8 classes.\n"]}],"source":["base_dir2 = os.path.join(os.getcwd(), \"Preprocessed_Task1\")\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","res_size = 224\n","classes_ls = [\"Unknown_and_Others\", \"Cars\", \"2_Wheeler\", \"Offroad\", \"Industrial\", \"Utility\", \"Service\", \"Commercial\"]\n","\n","train_datagen = ImageDataGenerator(height_shift_range=0.15, fill_mode='nearest',\n","                                   horizontal_flip=True, rescale=1.0/255., validation_split=0.15,\n","                                   width_shift_range=0.1, brightness_range=(0.3, 0.7), zoom_range=0.1,\n","                                  rotation_range = 10, shear_range=0.1)\n","\n","train_generator = train_datagen.flow_from_directory(\n","                  base_dir2, target_size=(res_size, res_size), color_mode='rgb',\n","                  classes = classes_ls,\n","                  class_mode='categorical', batch_size=32, shuffle=True,\n","                  subset=\"training\", interpolation='lanczos')\n","\n","val_generator = train_datagen.flow_from_directory(\n","                base_dir2, target_size=(res_size, res_size), color_mode='rgb',\n","                classes = classes_ls,\n","                class_mode='categorical', batch_size=32, shuffle=True,\n","                subset=\"validation\", interpolation='lanczos')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LqeF9xvqb5Zv"},"outputs":[],"source":["#Compiling the model\n","from tensorflow.keras.optimizers import Adam\n","opt = Adam(learning_rate = 4e-3)\n","loss = tensorflow.keras.losses.CategoricalCrossentropy()\n","\n","resnetv2_model.compile(optimizer = opt, \n","                       loss = loss, \n","                       metrics = [\"accuracy\"])\n","\n","lor = tf.keras.callbacks.ReduceLROnPlateau(\n","    monitor='val_accuracy', factor=0.25, patience=4, verbose=1,\n","    mode='auto', min_delta=0.02, min_lr=1e-12)\n","\n","checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","    \"ResNetV2_training/training-{epoch:03d}.h5\", monitor='val_accuracy', verbose=1, save_best_only=True,\n","    save_weights_only = False, save_freq='epoch') \n","\n","with tf.device('/gpu:0'):\n","    history2 = resnetv2_model.fit(\n","           train_generator,\n","           validation_data = val_generator,\n","           epochs = 25, \n","           verbose = 1, shuffle = True,\n","           callbacks = [lor, checkpoint])"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"f__KsJ3cb5Zv"},"source":["#### Training an EfficientNetv2 B0 on preprocessed data - Tensorflow Hub"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"lrQ-YsThb5Zv"},"outputs":[],"source":["effb0_size = 224\n","do_fine_tuning = True\n","base_model = hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/classification/2\",\n","                            trainable = do_fine_tuning)\n","\n","effv2b0_model = tf.keras.Sequential([\n","    tf.keras.layers.InputLayer(input_shape=(effb0_size, effb0_size,3)),\n","    base_model,\n","\n","    # Add a fully connected layer \n","    tf.keras.layers.Dense(512, activation='relu', use_bias=True,\n","                 kernel_initializer = tf.keras.initializers.HeNormal(),\n","                 bias_initializer='zeros'),\n","    #tf.keras.layers.BatchNormalization(),\n","\n","    # Add another fully connected layer \n","    tf.keras.layers.Dense(256, activation='relu', use_bias=True,\n","                 kernel_initializer = tf.keras.initializers.HeNormal(),\n","                 bias_initializer='zeros'),\n","    #tf.keras.layers.BatchNormalization(),\n","\n","    # Add another fully connected layer \n","    tf.keras.layers.Dense(64, activation='relu', use_bias=True,\n","                 kernel_initializer = tf.keras.initializers.HeNormal(),\n","                 bias_initializer='zeros'),\n","    \n","    # Add a final sigmoid layer for classification\n","    tf.keras.layers.Dense(8, activation = \"softmax\")])         \n","\n","effv2b0_model.build((None, effb0_size, effb0_size, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true},"tags":[],"id":"TBjoNQd4b5Zv"},"outputs":[],"source":["#Compiling the model\n","from tensorflow.keras.optimizers import Adam\n","\n","opt = Adam(learning_rate=5e-3)\n","loss = tensorflow.keras.losses.CategoricalCrossentropy()\n","\n","effv2b0_model.compile(optimizer = opt, \n","                       loss = loss, \n","                       metrics = [\"accuracy\"])\n","\n","lor = tf.keras.callbacks.ReduceLROnPlateau(\n","    monitor='val_accuracy', factor=0.3, patience=4, verbose=1,\n","    mode='auto', min_delta=0.01, min_lr=1e-13)\n","\n","checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","    \"Efficientv2B0_training-{epoch:03d}.h5\", monitor='val_accuracy', verbose=1, save_best_only=True,\n","    save_weights_only = False, save_freq='epoch')\n","\n","history2 = effv2b0_model.fit(\n","            train_generator,\n","            validation_data = val_generator,\n","            epochs = 60, \n","            verbose = 1, shuffle=True,\n","            callbacks = [lor, checkpoint])"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"RZFnSdLub5Zv"},"outputs":[],"source":["acc = history2.history['accuracy']\n","val_acc = history2.history['val_accuracy']\n","loss = history2.history['loss']\n","val_loss = history2.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'r', label='Training accuracy')\n","plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.legend(loc=0)\n","plt.figure()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"GdzyOW2ab5Zw"},"source":["### Section III: Evaluating model performance"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"aBK5PqIjb5Zw"},"source":["#### ResNetV250 model"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true},"tags":[],"id":"nSxWGouQb5Zw"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","checkpoint_path = \"ResNetV2_training/training_ResNetV2-046.h5\"\n","test_resnetv250_model = load_model(checkpoint_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true},"tags":[],"id":"zFt097zub5Z1","outputId":"e9ad8989-5e43-42cf-bb3b-aabbf8680765"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1132 images belonging to 8 classes.\n","36/36 [==============================] - 58s 2s/step\n"]},{"data":{"text/plain":["<tf.Tensor: shape=(8, 8), dtype=int32, numpy=\n","array([[  3,   0,   0,   0,   0,   0,   0,   2],\n","       [  5, 628,   0,  24,   8,  33,   1,  21],\n","       [  0,   7,  13,   8,   0,   6,   0,   0],\n","       [  0,  46,   0,  22,   0,  12,   1,   0],\n","       [  0,   0,   0,   0,  20,   0,   0,   0],\n","       [  1,  84,   5,  13,   0,  87,   1,   3],\n","       [  0,   4,   1,   0,   2,   5,  30,   1],\n","       [  0,  15,   0,   0,   1,   2,   0,  17]], dtype=int32)>"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["val_generator = train_datagen.flow_from_directory(\n","                base_dir2, target_size=(res_size, res_size), color_mode='rgb',\n","                classes = classes_ls,\n","                class_mode='categorical', batch_size=32, shuffle=False,\n","                subset=\"validation\", interpolation='bilinear')\n","\n","predictions = test_resnetv250_model.predict(val_generator, verbose=1, use_multiprocessing=True)\n","y_true = val_generator.classes\n","tf.math.confusion_matrix(\n","    y_true, np.argmax(predictions, axis=1), num_classes=8)\n","classes_ls = [\"Unknown_and_Others\", \"Cars\", \"2_Wheeler\", \"Offroad\", \"Industrial\", \"Utility\", \"Service\", \"Commercial\"]"]},{"cell_type":"markdown","metadata":{"id":"vNAif-gRb5Z1"},"source":["Inferences --\n","* Cars being confused with Utility class vehicles (Utility & vans)\n","* Cars being confused with OffRoad class\n","* Cars being confused with Commercial vehicles"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-HnaXlfb5Z1","outputId":"e8f67aa9-5a4d-4f51-a09a-df026e5ec881"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1132 images belonging to 8 classes.\n"]},{"name":"stderr","output_type":"stream","text":["2021-12-11 03:14:35.978600: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8200\n"]},{"name":"stdout","output_type":"stream","text":["71/71 [==============================] - 158s 2s/step\n"]}],"source":["from tensorflow.keras.models import load_model\n","checkpoint_path = \"EfficientB4_training/training-017-0.9035-0.7473.h5\"\n","\n","with tf.device('/gpu:0'):\n","    test_effb4_model = load_model(checkpoint_path, custom_objects={'KerasLayer':hub.KerasLayer})\n","    effb4_size = 384\n","\n","    val_generator = train_datagen.flow_from_directory(\n","                      base_dir2, target_size=(effb4_size, effb4_size), color_mode='rgb',\n","                      classes = classes_ls,\n","                      class_mode='categorical', batch_size=16, shuffle=False,\n","                      subset=\"validation\", interpolation='lanczos')\n","\n","    predictions = test_effb4_model.predict(val_generator, verbose=1, use_multiprocessing=True)\n","    y_true = val_generator.classes\n","    \n","print(tf.math.confusion_matrix(\n","        y_true, np.argmax(predictions, axis=1), num_classes=8))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rRjPUu-mb5Z2"},"outputs":[],"source":["#Loading test dataset images\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","test_X384 = np.array(np.load(\"effv2s_test_x.npy\"))\n","test_datagen = ImageDataGenerator(rescale=1.0/255.)\n","\n","test_generator = test_datagen.flow(test_X384, batch_size = 16, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fPx9eKUb5Z2","outputId":"68ea4d43-ce45-4cba-822d-f7f800983708"},"outputs":[{"name":"stderr","output_type":"stream","text":["2021-12-11 03:51:02.648274: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8200\n"]},{"name":"stdout","output_type":"stream","text":["165/165 [==============================] - 39s 168ms/step\n","Dimensions of the predictions array: (2631, 8)\n","File_downloaded as .csv!!\n"]}],"source":["##Creating submission csv based on EffNetB4 training\n","#Transforming labels to appropriate classes for csv submission\n","def convert_label(label):\n","    if label == 3 or label == 4 or label == 5:\n","        return 2\n","    elif label == 6 or label == 7:\n","        return 0\n","    else:\n","        return label\n","        \n","from tensorflow.keras.models import load_model\n","test_files = np.load(\"test_files.npy\")\n","\n","def test_model_performance(model_path, csv_file_name, test_array):\n","    checkpoint_path = model_path\n","    with tf.device('/gpu:0'):\n","        test_model = load_model(checkpoint_path, custom_objects={'KerasLayer':hub.KerasLayer})\n","\n","        #Obtain predictions from the test data\n","        predictions = test_model.predict(test_generator, verbose=1)\n","        print(\"Dimensions of the predictions array: {}\".format(predictions.shape))\n","        assert predictions.shape[0] == np.array(test_array).shape[0]\n","        predictions_ls = [convert_label(label) for label in np.argmax(predictions, axis=1).tolist()]\n","\n","        #Creating a dataframe and saving it as output\n","        test_file = [fname[5:-10] for fname in test_files]\n","        data = {'guid/image': test_file, 'label': predictions_ls}\n","        output_df = pd.DataFrame(data)\n","        output_df.to_csv(csv_file_name, index=False)\n","        print(\"File_downloaded as .csv!!\")\n","\n","test_model_performance(model_path = \"EfficientB4_training/training-017-0.9035-0.7473.h5\",\n","                       csv_file_name = \"EffNetB4_preprocessed_Team11.csv\", \n","                       test_array = test_X384)"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"KU5S8Ty0b5Z2"},"source":["#### Performing Error analysis on test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPdN7EUdb5Z2"},"outputs":[],"source":["test_X = np.array(np.load(\"res_test_x.npy\"))\n","test_datagen = ImageDataGenerator(rescale=1.0/255., width_shift_range = 0.2,\n","                                  height_shift_range = 0.2, horizontal_flip = True)\n","\n","test_generator = test_datagen.flow(test_X, batch_size = 32, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_PgpEZ74b5Z2","outputId":"203f2776-294b-40a4-ac22-1940c9787c6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["83/83 [==============================] - 25s 298ms/step\n"]}],"source":["test_predictions = test_resnetv250_model.predict(test_generator, verbose=1, use_multiprocessing=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true},"tags":[],"id":"qYzkqqIMb5Z3"},"outputs":[],"source":["def test_multiple_images(nrows, ncols):\n","    ncols = ncols\n","    nrows = nrows\n","    fig = plt.gcf()\n","    fig.set_size_inches(14, 14)\n","    img_data = next(test_generator)[:16]\n","    \n","    for idx, img in tqdm_notebook(enumerate(img_data)):\n","        \n","        # Set up subplot; subplot indices start at 1\n","        sp = plt.subplot(nrows, ncols, idx + 1)\n","        sp.axis('Off') # Don't show axes (or gridlines)\n","        #img = plt.imread(img)\n","        plt.imshow(img)\n","        #Adding additional axis to predict using test model\n","        img = img[np.newaxis, :]\n","        #Prediction label\n","        var = test_resnetv250_model.predict(img)\n","        pred = np.argmax(var, axis=1)[0]\n","        sp.set_title('Model prediction: {} - {}'.format(pred, classes_ls[pred]))\n","\n","    plt.show()\n","\n","test_multiple_images(4, 4)"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"NJHlkQ7kb5Z3"},"source":["#### Training a fully tuned EfficientNetV2-S model "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B3QWotW_b5Z3","outputId":"f48da364-d3fe-4c0b-8bf6-e39663499004"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 6441 images belonging to 8 classes.\n","Found 1132 images belonging to 8 classes.\n"]}],"source":["base_dir2 = os.path.join(os.getcwd(), \"Preprocessed_Task1\")\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","effb4_size = 384\n","classes_ls = [\"Unknown_and_Others\", \"Cars\", \"2_Wheeler\", \"Offroad\", \"Industrial\", \"Utility\", \"Service\", \"Commercial\"]\n","\n","train_datagen = ImageDataGenerator(height_shift_range=0.15, fill_mode='nearest',\n","                                   horizontal_flip=True, rescale=1.0/255., validation_split=0.15,\n","                                   width_shift_range=0.15, brightness_range=(0.3, 0.7), zoom_range=0.1,\n","                                   rotation_range = 10, shear_range=0.1)\n","\n","train_generator = train_datagen.flow_from_directory(\n","                  base_dir2, target_size=(effb4_size, effb4_size), color_mode='rgb',\n","                  classes = classes_ls,\n","                  class_mode='categorical', batch_size=16, shuffle=True,\n","                  subset=\"training\", interpolation='lanczos')\n","\n","val_generator = train_datagen.flow_from_directory(\n","                base_dir2, target_size=(effb4_size, effb4_size), color_mode='rgb',\n","                classes = classes_ls,\n","                class_mode='categorical', batch_size=16, shuffle=True,\n","                subset=\"validation\", interpolation='lanczos')"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"g8tdlb8Sb5Z3"},"outputs":[],"source":["#Loading the EfficientNetv2s model\n","effv2s_size = 384\n","do_fine_tuning = True\n","base_model = hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2\",\n","                            trainable = do_fine_tuning)\n","\n","effv2s_model = tf.keras.Sequential([\n","    tf.keras.layers.InputLayer(input_shape=(effv2s_size, effv2s_size,3)),\n","    base_model,\n","\n","    # Add a fully connected layer \n","    tf.keras.layers.Dense(512, activation='relu', use_bias=True,\n","                 kernel_initializer = tf.keras.initializers.HeNormal(),\n","                 bias_initializer='zeros'),\n","    #tf.keras.layers.BatchNormalization(),\n","\n","    # Add another fully connected layer \n","    tf.keras.layers.Dense(256, activation='relu', use_bias=True,\n","                 kernel_initializer = tf.keras.initializers.HeNormal(),\n","                 bias_initializer='zeros'),\n","    #tf.keras.layers.BatchNormalization(),\n","    \n","    # Add another fully connected layer \n","    tf.keras.layers.Dense(128, activation='relu', use_bias=True,\n","                 kernel_initializer = tf.keras.initializers.HeNormal(),\n","                 bias_initializer='zeros'),\n","    #tf.keras.layers.BatchNormalization(),\n","\n","    # Add another fully connected layer \n","    tf.keras.layers.Dense(64, activation='relu', use_bias=True,\n","                 kernel_initializer = tf.keras.initializers.HeNormal(),\n","                 bias_initializer='zeros'),\n","    \n","    # Add a final sigmoid layer for classification\n","    tf.keras.layers.Dense(8, activation = \"softmax\")])         \n","\n","effv2s_model.build((None, effv2s_size, effv2s_size, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMkeMFt-b5Z3"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","checkpoint_path = \"EfficientV2s_training/training-017-0.9035-0.7473.h5\"\n","effb4_iter2_model = load_model(checkpoint_path, custom_objects={'KerasLayer':hub.KerasLayer}) \n","\n","#Compiling the model\n","from tensorflow.keras.optimizers import Adam\n","\n","opt = Adam(learning_rate=1e-5)\n","loss = tensorflow.keras.losses.CategoricalCrossentropy()\n","\n","with tf.device('/gpu:0'):\n","    effb4_iter2_model.compile(optimizer = opt, \n","                       loss = loss, \n","                       metrics = [\"accuracy\"])\n","\n","    lor = tf.keras.callbacks.ReduceLROnPlateau(\n","        monitor='val_accuracy', factor=0.25, patience=4, verbose=1,\n","        mode='auto', min_delta=0.01, min_lr=1e-13)\n","\n","    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","        \"EfficientV2s_training/training-{epoch:03d}-{val_loss:.4f}-{val_accuracy:.4f}.h5\",\n","        monitor='val_accuracy', verbose=1, save_best_only=True,\n","        save_weights_only = False, save_freq='epoch')\n","\n","    history3 = effb4_iter2_model.fit(\n","            train_generator,\n","            validation_data = val_generator,\n","            epochs = 35, \n","            verbose = 1, shuffle=True,\n","            callbacks = [lor, checkpoint])"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"m-Gj1T3Ub5Z3"},"source":["#### Model EffNetV2s evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Bgvvgaqb5Z4","outputId":"5d57ab92-46ed-490c-80dd-aba11a7db39c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1132 images belonging to 8 classes.\n","71/71 [==============================] - 124s 2s/step\n","tf.Tensor(\n","[[  0   0   0   0   0   0   0   5]\n"," [  0 659   1  21   0  21   0  18]\n"," [  0   5  15   8   0   6   0   0]\n"," [  0  32   1  30   0  18   0   0]\n"," [  0   0   0   0  20   0   0   0]\n"," [  0  89   2  27   0  75   0   1]\n"," [  0   5   0   0   0   4  34   0]\n"," [  0  20   0   3   0   2   0  10]], shape=(8, 8), dtype=int32)\n"]}],"source":["from tensorflow.keras.models import load_model\n","checkpoint_path = \"EfficientV2s_training/training-007-0.9817-0.7482.h5\"\n","\n","with tf.device('/gpu:0'):\n","    test_effb4_model = load_model(checkpoint_path, custom_objects={'KerasLayer':hub.KerasLayer})\n","    effb4_size = 384\n","\n","    val_generator = train_datagen.flow_from_directory(\n","                      base_dir2, target_size=(effb4_size, effb4_size), color_mode='rgb',\n","                      classes = classes_ls,\n","                      class_mode='categorical', batch_size=16, shuffle=False,\n","                      subset=\"validation\", interpolation='lanczos')\n","\n","    predictions = test_effb4_model.predict(val_generator, verbose=1, use_multiprocessing=True)\n","    y_true = val_generator.classes\n","    \n","print(tf.math.confusion_matrix(\n","        y_true, np.argmax(predictions, axis=1), num_classes=8))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iieYoGZAb5Z4","outputId":"53616bb9-402b-4ee2-cebe-f481a0c736fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1132 images belonging to 8 classes.\n","71/71 [==============================] - 125s 2s/step\n","tf.Tensor(\n","[[  0   0   0   0   0   0   0   5]\n"," [  0 653   2  26   2  21   0  16]\n"," [  0   6  19   3   0   6   0   0]\n"," [  0  38   0  27   0  16   0   0]\n"," [  0   0   0   0  19   0   0   1]\n"," [  0  82   3  21   0  88   0   0]\n"," [  0   2   0   1   0   6  34   0]\n"," [  0  19   0   2   0   4   0  10]], shape=(8, 8), dtype=int32)\n"]}],"source":["from tensorflow.keras.models import load_model\n","checkpoint_path = \"EfficientV2s_training/training-017-0.9035-0.7473.h5\"\n","\n","with tf.device('/gpu:0'):\n","    test_effb4_model = load_model(checkpoint_path, custom_objects={'KerasLayer':hub.KerasLayer})\n","    effb4_size = 384\n","\n","    val_generator = train_datagen.flow_from_directory(\n","                      base_dir2, target_size=(effb4_size, effb4_size), color_mode='rgb',\n","                      classes = classes_ls,\n","                      class_mode='categorical', batch_size=16, shuffle=False,\n","                      subset=\"validation\", interpolation='lanczos')\n","\n","    predictions = test_effb4_model.predict(val_generator, verbose=1, use_multiprocessing=True)\n","    y_true = val_generator.classes\n","    \n","print(tf.math.confusion_matrix(\n","        y_true, np.argmax(predictions, axis=1), num_classes=8))"]},{"cell_type":"markdown","metadata":{"id":"Annx1E_Ib5Z4"},"source":["#### Training a fully tuned EfficientNEtV2-S model on subset data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Q4d6Mkcb5Z4","outputId":"9ff98b2e-ddf0-49ad-e163-371a92ed8da6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 5649 images belonging to 3 classes.\n","Found 995 images belonging to 3 classes.\n"]}],"source":["base_dir2 = os.path.join(os.getcwd(), \"Preprocessed_subset_Task1\")\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","effv2s_size = 384\n","classes_ls = [\"Offroad\", \"Cars\", \"Utility\"]\n","\n","train_datagen = ImageDataGenerator(height_shift_range=0.15, fill_mode='nearest',\n","                                   horizontal_flip=True, rescale=1.0/255., validation_split=0.15,\n","                                   width_shift_range=0.15, brightness_range=(0.3, 0.7), zoom_range=0.1,\n","                                   rotation_range = 10, shear_range=0.1)\n","\n","train_generator = train_datagen.flow_from_directory(\n","                  base_dir2, target_size=(effv2s_size, effv2s_size), color_mode='rgb',\n","                  classes = classes_ls,\n","                  class_mode='categorical', batch_size=8, shuffle=True,\n","                  subset=\"training\", interpolation='lanczos')\n","\n","val_generator = train_datagen.flow_from_directory(\n","                base_dir2, target_size=(effv2s_size, effv2s_size), color_mode='rgb',\n","                classes = classes_ls,\n","                class_mode='categorical', batch_size=8, shuffle=True,\n","                subset=\"validation\", interpolation='lanczos')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UvHRqAGab5Z4"},"outputs":[],"source":["#Loading the EfficientNetv2s model\n","effv2s_size = 384\n","do_fine_tuning = True\n","base_model = hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2\",\n","                            trainable = do_fine_tuning)\n","\n","effv2s_model = tf.keras.Sequential([\n","    tf.keras.layers.InputLayer(input_shape=(effv2s_size, effv2s_size,3)),\n","    base_model,\n","\n","    # Add a fully connected layer \n","    tf.keras.layers.Dense(512, activation='relu', use_bias=True,\n","                 kernel_initializer = tf.keras.initializers.HeNormal(),\n","                 bias_initializer='zeros'),\n","    tf.keras.layers.Dropout(rate = 0.15),\n","    \n","    # Add another fully connected layer \n","    tf.keras.layers.Dense(256, activation='relu', use_bias=True,\n","                 kernel_initializer = tf.keras.initializers.HeNormal(),\n","                 bias_initializer='zeros'),\n","    tf.keras.layers.Dropout(rate = 0.15),\n","\n","    # Add another fully connected layer \n","    tf.keras.layers.Dense(128, activation='relu', use_bias=True,\n","                 kernel_initializer = tf.keras.initializers.HeNormal(),\n","                 bias_initializer='zeros'),\n","    \n","    # Add a final sigmoid layer for classification\n","    tf.keras.layers.Dense(3, activation = \"softmax\")])         \n","\n","effv2s_model.build((None, effv2s_size, effv2s_size, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true},"tags":[],"id":"ZoUcw0JAb5Z4"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","from tensorflow.keras.optimizers import Adam\n","checkpoint_path = \"EfficientV2s_subset_training/training-016-0.7580-0.7930.h5\"\n","\n","with tf.device('/gpu:0'):\n","    effv2s_model = load_model(checkpoint_path, custom_objects={'KerasLayer':hub.KerasLayer})\n","    \n","    #Compiling the model\n","    opt = Adam(learning_rate=1e-6)\n","    loss = tensorflow.keras.losses.CategoricalCrossentropy()\n","\n","    effv2s_model.compile(optimizer = opt, \n","                       loss = loss, \n","                       metrics = [\"accuracy\"])\n","\n","    lor = tf.keras.callbacks.ReduceLROnPlateau(\n","        monitor='val_accuracy', factor=0.25, patience=3, verbose=1,\n","        mode='auto', min_delta=0.01, min_lr=1e-13)\n","\n","    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","        \"EfficientV2s_subset_training/training-{epoch:03d}-{val_loss:.4f}-{val_accuracy:.4f}.h5\",\n","        monitor='val_accuracy', verbose=1, save_best_only=True,\n","        save_weights_only = False, save_freq='epoch')\n","\n","    history3 = effv2s_model.fit(\n","            train_generator,\n","            validation_data = val_generator,\n","            epochs = 15, \n","            verbose = 1, shuffle=True,\n","            callbacks = [lor, checkpoint])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9Wm6ogwb5Z5","outputId":"d62a35a8-3a7b-46fd-96e1-e8121a229c3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 995 images belonging to 3 classes.\n"]},{"name":"stderr","output_type":"stream","text":["2021-12-12 15:48:59.222530: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8200\n"]},{"name":"stdout","output_type":"stream","text":["125/125 [==============================] - 147s 1s/step\n","tf.Tensor(\n","[[ 21  39  21]\n"," [ 15 674  31]\n"," [ 10  97  87]], shape=(3, 3), dtype=int32)\n"]}],"source":["from tensorflow.keras.models import load_model\n","checkpoint_path = \"EfficientV2s_subset_training/training-001-0.7265-0.7889.h5\"\n","\n","with tf.device('/gpu:0'):\n","    test_effv2_model = load_model(checkpoint_path, custom_objects={'KerasLayer':hub.KerasLayer})\n","    effv2s_size = 384\n","\n","    val_generator = train_datagen.flow_from_directory(\n","                      base_dir2, target_size=(effv2s_size, effv2s_size), color_mode='rgb',\n","                      classes = classes_ls,\n","                      class_mode='categorical', batch_size=8, shuffle=False,\n","                      subset=\"validation\", interpolation='lanczos')\n","\n","    predictions = test_effv2_model.predict(val_generator, verbose=1, use_multiprocessing=True)\n","    y_true = val_generator.classes\n","    \n","print(tf.math.confusion_matrix(\n","        y_true, np.argmax(predictions, axis=1), num_classes=3))"]},{"cell_type":"markdown","metadata":{"id":"gAlr80akb5Z5"},"source":["**Inferences: Comparison between 0.7889 and 0.7930 models**\n","* *Slightly better on Utility class*\n","* *Better on cars class*\n","* *Worse on offroad class*"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"mr9kY7oFb5Z5","outputId":"90f8eb33-dd8e-45a7-c906-a4be6ccb87d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 995 images belonging to 3 classes.\n","125/125 [==============================] - 97s 768ms/step\n","tf.Tensor(\n","[[ 32  36  13]\n"," [ 27 660  33]\n"," [ 23  86  85]], shape=(3, 3), dtype=int32)\n"]}],"source":["from tensorflow.keras.models import load_model\n","checkpoint_path = \"EfficientV2s_subset_training/training-016-0.7580-0.7930.h5\"\n","\n","with tf.device('/gpu:0'):\n","    test_effv2_model = load_model(checkpoint_path, custom_objects={'KerasLayer':hub.KerasLayer})\n","    effv2s_size = 384\n","\n","    val_generator = train_datagen.flow_from_directory(\n","                      base_dir2, target_size=(effv2s_size, effv2s_size), color_mode='rgb',\n","                      classes = classes_ls,\n","                      class_mode='categorical', batch_size=8, shuffle=False,\n","                      subset=\"validation\", interpolation='lanczos')\n","\n","    predictions = test_effv2_model.predict(val_generator, verbose=1, use_multiprocessing=True)\n","    y_true = val_generator.classes\n","    \n","print(tf.math.confusion_matrix(\n","        y_true, np.argmax(predictions, axis=1), num_classes=3))"]},{"cell_type":"markdown","metadata":{"id":"5ccyIWk3b5Z5"},"source":["### Testing performance using Stacked Ensemble soft voting classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"grRPsHWnb5Z5"},"outputs":[],"source":["#Transforming labels to appropriate classes for csv submission\n","def convert_label(label):\n","    \"\"\"\n","    Takes an input predicted label from model and converts it into \n","    the final class label 0/1/2\n","    \"\"\"\n","    if label == 3 or label == 4 or label == 5:\n","        return 2\n","    elif label == 6 or label == 7:\n","        return 0\n","    else:\n","        return label\n","\n","def sub_model_label(label):\n","    \"\"\"\n","    Takes an input predicted label for stacked sub classifier and \n","    outputs class label as itself or 2\n","    \"\"\"\n","    if label == 0:\n","        return 2\n","    else:\n","        return label\n","\n","#Reference classes list -- Preprocessed train set into 8 sub directories\n","classes_ls = [\"Unknown_and_Others\", \"Cars\", \"2_Wheeler\", \"Offroad\", \"Industrial\", \"Utility\", \"Service\", \"Commercial\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXrYPFoTb5Z5"},"outputs":[],"source":["#Defining custom test generator with soft voting\n","def custom_augmentation_testgen(test_npy_file, \n","                                saved_full_model_path,\n","                                saved_subset_model_path1,\n","                                saved_subset_model_path2,\n","                                test_files,\n","                                csv_file_name):\n","  \n","    \"\"\"\n","    Inputs:\n","    test_npy_file: Input numpy file which contains test images in form (batch, size, size, channel_size)\n","    Use np.load(\"test_npy_file\") to use it for custom generator\n","\n","    saved_full_model_path: Full 8 classes classifier model path\n","\n","    saved_subset_model_path1: Cars, Offroad and Utility class classfier model path\n","    Preferred when class label is Cars or Utility\n","\n","    saved_subset_model_path2: Second Cars, Offroad and Utility class classfier model path \n","    Preferred when class label is Offroad\n","\n","    test_files: Test files loaded using glob from the directory -- Used to create column in DataFrame\n","\n","    csv_file_name: Output csv file name to store the submission \n","\n","    Output: Submission csv file to upload on Kaggle\n","    \"\"\"\n","\n","    #Aggregating predictions on each test image\n","    prediction_ls = []\n","    \n","    #Used for prioritizing between stacked Image Classifier and Object Detection model -- 0/1\n","    flag_label = []\n","\n","    #Loading saved full model\n","    checkpoint_path = saved_full_model_path\n","    test_model_1 = load_model(checkpoint_path, custom_objects={'KerasLayer':hub.KerasLayer})\n","\n","    #Loading saved subset model 1 -- Trained on Cars, Offroad and Utility class only \n","    #Preferred when class is Cars or Utility\n","    checkpoint_path2 = saved_subset_model_path1\n","    test_model_2 = load_model(checkpoint_path2, custom_objects={'KerasLayer':hub.KerasLayer})\n","\n","    #Loading saved subset model 2 -- Trained on Cars, Offroad and Utility class only\n","    #Preferred when class is Offroad\n","    checkpoint_path3 = saved_subset_model_path2\n","    test_model_3 = load_model(checkpoint_path3, custom_objects={'KerasLayer':hub.KerasLayer})\n","\n","    for img in tqdm_notebook(test_npy_file):\n","        #Stacking 6 rescaled images - the original, random shift 0.1*x & 0.1*y, random rotation 10 degrees,\n","        #                             random brightness, random zoom 0.1 and random shear - intensity 8\n","        batch_img = np.vstack((img[np.newaxis, :]/255,\n","                          \n","                          tf.keras.preprocessing.image.random_shift(\n","                          img, wrg = 0.1, hrg = 0.1, row_axis=0,\n","                          col_axis=1, channel_axis=2)[np.newaxis, :]/255, \n","                          \n","                          tf.keras.preprocessing.image.random_rotation(\n","                          img, rg = 10, row_axis=0,\n","                          col_axis=1, channel_axis=2)[np.newaxis, :]/255,\n","\n","                          tf.keras.preprocessing.image.random_brightness(\n","                          img, brightness_range = (0.3,0.7))[np.newaxis, :]/255,\n","\n","                          tf.keras.preprocessing.image.random_zoom(\n","                          img, zoom_range = (0.1,0.1), row_axis=0,\n","                          col_axis=1, channel_axis=2)[np.newaxis, :]/255, \n","\n","                          tf.keras.preprocessing.image.random_shear(\n","                          img, intensity = 8, row_axis=0,\n","                          col_axis=1, channel_axis=2)[np.newaxis, :]/255                                   \n","                          ))\n","    \n","        #Checking whether the batch_img is valid or not\n","        assert batch_img.shape[0] == 6\n","\n","        #Obtain predictions from the batch Image data\n","        predictions = test_model_1.predict(batch_img)\n","        assert predictions.shape[0] == batch_img.shape[0]\n","\n","        #Performing soft voting based on probabilities -- single classifier\n","        final_label = np.argmax(np.average(predictions, axis=0))\n","        img_class = classes_ls[final_label]\n","\n","        #Building stacking classifier if predicted is either Cars, Offroad or Utility class \n","        if img_class == \"Cars\" or img_class == \"Utility\":\n","            #Predicting using seperate subset classifier 1 trained on these 3 classes \n","            pred = test_model_2.predict(batch_img)\n","            assert pred.shape[0] == batch_img.shape[0]\n","            \n","\n","            #Implementing soft voting and appending the predicted label\n","            label = np.argmax(np.average(pred, axis=0))\n","            prediction_ls.append(sub_model_label(label))\n","            flag_label.append(1)\n","\n","        elif img_class == \"Offroad\":\n","            #Predicting using seperate subset classifier 1 trained on these 3 classes \n","            pred = test_model_3.predict(batch_img)\n","            assert pred.shape[0] == batch_img.shape[0]\n","\n","            #Implementing soft voting and appending the predicted label\n","            label = np.argmax(np.average(pred, axis=0))\n","            prediction_ls.append(sub_model_label(label))\n","            flag_label.append(1)\n","\n","        else:\n","            prediction_ls.append(convert_label(final_label))\n","            flag_label.append(0)\n","\n","    #Creating a dataframe and saving it as output\n","    assert len(prediction_ls) == test_npy_file.shape[0]\n","\n","    #Based on submission regulations\n","    image_file = [fname[5:-10] for fname in test_files]\n","    data = {'guid/image': image_file, 'label': prediction_ls, 'flag':flag_label}\n","  \n","    #Creating the pandas DataFrame and saving it\n","    output_df = pd.DataFrame(data)\n","    output_df.to_csv(csv_file_name, index=False)\n","    print(\"File_downloaded as .csv!!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rISA-cMQb5Z6"},"outputs":[],"source":["#Running the function\n","test_npy_file = np.load(\"effv2s_test_x.npy\") #Contains each image of dim (384,384,3) -- Input for EfficientV2s model\n","model_path = \"EfficientV2s_training/training-017-0.9035-0.7473.h5\"\n","model_path2 = \"EfficientV2s_subset_training/training-001-0.7265-0.7889.h5\"\n","model_path3 = \"EfficientV2s_subset_training/training-016-0.7580-0.7930.h5\"\n","test_files = sorted(glob('test/*/*_image.jpg'))\n","file_name = \"Stacked_EfficientV2s_softVoting_Team11_flag.csv\"\n","\n","custom_augmentation_testgen(test_npy_file = test_npy_file,\n","                            saved_full_model_path = model_path,\n","                            saved_subset_model_path1 = model_path2,\n","                            saved_subset_model_path2 = model_path3,\n","                            test_files = test_files,\n","                            csv_file_name = file_name)"]}],"metadata":{"colab":{"collapsed_sections":["cH_b_4jUb5Zg","c_--UxkDb5Zj","vUy8yak0b5Zm","pxNUNklVb5Zr","J_c0mXmPb5Zs","eUdo6xn-b5Zt","Sdu2qiA1b5Zu","f__KsJ3cb5Zv","aBK5PqIjb5Zw","KU5S8Ty0b5Z2","NJHlkQ7kb5Z3","m-Gj1T3Ub5Z3","Annx1E_Ib5Z4","5ccyIWk3b5Z5"],"name":"ROB535_Perception_GCP.ipynb","provenance":[]},"environment":{"kernel":"python3","name":"tf2-gpu.2-7.m87","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":0}